def pandas_read_bronze_data(context: AssetExecutionContext, minio: MinIOResource,bronze_path):
    context.log.info(f"ğŸ“– æ­£åœ¨ä½¿ç”¨pandasè¯»å–Bronzeå±‚Delta Lakeæ•°æ®: {bronze_path}")
    
    try:
        from deltalake import DeltaTable
        
        # ç¡®ä¿è·¯å¾„æ ¼å¼æ­£ç¡®
        if not bronze_path.endswith('/'):
            bronze_path += '/'
        
        context.log.info(f"ğŸ” å°è¯•è¯»å–Delta Lakeè¡¨: {bronze_path}")
        
        # è¯»å–Delta Lakeè¡¨
        delta_table = DeltaTable(bronze_path, storage_options=minio.get_storage_options())
        bronze_data = delta_table.to_pandas()
        
        context.log.info(f"âœ… Bronzeå±‚æ•°æ®è¯»å–æˆåŠŸï¼Œå…± {len(bronze_data)} æ¡è®°å½•")
        context.log.info(f"ğŸ“Š æ•°æ®åˆ—å: {list(bronze_data.columns)}")
        
    except Exception as e:
        context.log.error(f"âŒ Bronzeå±‚æ•°æ®è¯»å–å¤±è´¥: {e}")
        context.log.error("è¯·ç¡®ä¿Bronzeå±‚æ•°æ®å·²æ­£ç¡®æ‘„å–")
        raise
    return bronze_data

def identify_key_fields_exist(context: AssetExecutionContext, bronze_data, required_fields):
    missing_fields = [field for field in required_fields if field not in bronze_data.columns]
    if missing_fields:
        context.log.error(f"âŒ ç¼ºå°‘å…³é”®å­—æ®µ: {missing_fields}")
        raise ValueError(f"ç¼ºå°‘å…³é”®å­—æ®µ: {missing_fields}")
    context.log.info("âœ… æ‰€æœ‰å…³é”®å­—æ®µéƒ½å­˜åœ¨")

def process_datetime_field(bronze_data, source_field, target_field, cleaned_data, context):
    """
    å¤„ç†æ—¥æœŸæ—¶é—´å­—æ®µ
    
    Args:
        bronze_data: åŸå§‹æ•°æ®
        source_field: æºå­—æ®µå
        target_field: ç›®æ ‡å­—æ®µå
        cleaned_data: æ¸…æ´—åçš„æ•°æ®
        context: ä¸Šä¸‹æ–‡å¯¹è±¡
    
    Returns:
        cleaned_data: æ›´æ–°åçš„æ¸…æ´—æ•°æ®
    """
    # æ—¥æœŸæ—¶é—´å­—æ®µå¤„ç† - æ ¼å¼åŒ–ä¸ºæŒ‡å®šæ ¼å¼
    datetime_series = pd.to_datetime(
        bronze_data[source_field], errors='coerce'
    ).dt.floor('S')
    
    # æ ¼å¼åŒ–ä¸ºæŒ‡å®šæ ¼å¼çš„å­—ç¬¦ä¸²ï¼Œç¡®ä¿ä¸åŒ…å«éæ³•å­—ç¬¦
    cleaned_data[target_field] = datetime_series.dt.strftime('%Y/%m/%d %H:%M:%S')
    
    # ç¡®ä¿æ—¶é—´æ ¼å¼æ­£ç¡®ï¼Œå¤„ç†å¯èƒ½çš„å¼‚å¸¸å€¼
    cleaned_data[target_field] = cleaned_data[target_field].apply(
        lambda x: x if pd.isna(x) or (isinstance(x, str) and len(x) > 0) else "æœªçŸ¥æ—¶é—´"
    )
    
    return cleaned_data

def process_numeric_field(bronze_data, source_field, target_field,numeric_fields, cleaned_data, context):
    """
    å¤„ç†æ•°å€¼å­—æ®µ
    
    Args:
        bronze_data: åŸå§‹æ•°æ®
        source_field: æºå­—æ®µå
        target_field: ç›®æ ‡å­—æ®µå
        cleaned_data: æ¸…æ´—åçš„æ•°æ®
        context: ä¸Šä¸‹æ–‡å¯¹è±¡
    
    Returns:
        cleaned_data: æ›´æ–°åçš„æ¸…æ´—æ•°æ®
    """
    # æ•°å€¼å­—æ®µå¤„ç†
    default_value = 0.000
    cleaned_data[target_field] = pd.to_numeric(
        bronze_data[source_field], errors='coerce'
    ).fillna(default_value)
    
    # æ£€æŸ¥è´Ÿæ•°å€¼
    negative_mask = cleaned_data[target_field] < 0
    negative_count = negative_mask.sum()
    
    if negative_count > 0:
        context.log.warning(f"âš ï¸ å‘ç° {negative_count} æ¡è®°å½•çš„ {source_field} ä¸ºè´Ÿæ•°")
        # åˆ é™¤è¯¥å­—æ®µä¸ºè´Ÿæ•°çš„è®°å½•
        cleaned_data = cleaned_data[~negative_mask]
        context.log.info(f"âœ… å·²åˆ é™¤ {negative_count} æ¡ {source_field} ä¸ºè´Ÿæ•°çš„è®°å½•")
    
    # ç‰¹æ®Šå¤„ç†å­—æ®µ
    if source_field in numeric_fields:
        cleaned_data[target_field] = cleaned_data[target_field].round(3)
        context.log.info(f"âœ… {source_field} å­—æ®µå·²å¤„ç†ï¼Œä¿ç•™ä¸‰ä½å°æ•°")
    
    return cleaned_data

def process_string_field(bronze_data, source_field, target_field, cleaned_data):
    """
    å¤„ç†å­—ç¬¦ä¸²å­—æ®µ
    
    Args:
        bronze_data: åŸå§‹æ•°æ®
        source_field: æºå­—æ®µå
        target_field: ç›®æ ‡å­—æ®µå
        cleaned_data: æ¸…æ´—åçš„æ•°æ®
    
    Returns:
        cleaned_data: æ›´æ–°åçš„æ¸…æ´—æ•°æ®
    """
    cleaned_data[target_field] = bronze_data[source_field].fillna('').astype(str)
    return cleaned_data

def process_field_mapping(bronze_data, field_mapping, datetime_fields, numeric_fields, cleaned_data, context):
    """
    å¤„ç†å­—æ®µæ˜ å°„
    
    Args:
        bronze_data: åŸå§‹æ•°æ®
        field_mapping: å­—æ®µæ˜ å°„å­—å…¸
        datetime_fields: æ—¥æœŸæ—¶é—´å­—æ®µåˆ—è¡¨
        numeric_fields: æ•°å€¼å­—æ®µåˆ—è¡¨
        cleaned_data: æ¸…æ´—åçš„æ•°æ®
        context: ä¸Šä¸‹æ–‡å¯¹è±¡
    
    Returns:
        cleaned_data: æ›´æ–°åçš„æ¸…æ´—æ•°æ®
    """
    for source_field, target_field in field_mapping.items():
        if source_field in bronze_data.columns:
            if source_field in datetime_fields:
                cleaned_data = process_datetime_field(
                    bronze_data, source_field, target_field, cleaned_data, context
                )
            elif source_field in numeric_fields:
                cleaned_data = process_numeric_field(
                    bronze_data, source_field, target_field, numeric_fields, cleaned_data, context
                )
            else:
                cleaned_data = process_string_field(
                    bronze_data, source_field, target_field, cleaned_data
                )
        else:
            context.log.warning(f"âš ï¸ æºå­—æ®µ '{source_field}' åœ¨bronzeæ•°æ®ä¸­ä¸å­˜åœ¨")
    
    return cleaned_data

def process_special_bo_fields(bronze_data, field_mapping, cleaned_data, special_process_fields, context):
    for source_field, target_field in field_mapping.items():
        if source_field in special_process_fields:
            cleaned_data[target_field] = bronze_data[source_field].astype(str)
            # æ›¿æ¢ç©ºå€¼ã€'*'ã€'nan'ã€'None'ç­‰ä¸º'NULL'
            cleaned_data[target_field] = cleaned_data[target_field].replace([
                '', 'nan', 'None', 'NULL', '*', 'null', 'NaN', 'NAN'
            ], 'NULL')
    context.log.info(f"âœ… {source_field} å­—æ®µç‰¹æ®Šå¤„ç†å®Œæˆï¼šç©ºå€¼å’Œ'*'å·²æ›¿æ¢ä¸ºNULL")
    return cleaned_data

def process_add_field(bronze_data, field_mapping, cleaned_data, add_field, add_field_context, context):
    if add_field not in cleaned_data.columns:
        cleaned_data[add_field] = add_field_context
        context.log.info(f"âœ… æ·»åŠ  {add_field} å­—æ®µï¼Œé»˜è®¤å€¼è®¾ç½®ä¸º {add_field_context}")
    return cleaned_data

def time_logic_check(cleaned_data,start_time,end_time,context):
    create_times = pd.to_datetime(cleaned_data[start_time], errors='coerce')
    update_times = pd.to_datetime(cleaned_data[end_time], errors='coerce')
    invalid_time_logic = (create_times > update_times) & (create_times.notna()) & (update_times.notna())
    invalid_records_count = invalid_time_logic.sum()
    
    if invalid_records_count > 0:
        context.log.warning(f"âš ï¸ å‘ç° {invalid_records_count} æ¡è®°å½•çš„CREATE_TIMEå¤§äºUPDATE_TIMEï¼Œå°†è¢«åˆ é™¤")
        
        # æ˜¾ç¤ºè¢«åˆ é™¤çš„è®°å½•ä¿¡æ¯
        invalid_indices = invalid_time_logic[invalid_time_logic].index
        for idx in invalid_indices[:5]:  # åªæ˜¾ç¤ºå‰5æ¡
            context.log.warning(f"  åˆ é™¤è®°å½• {idx}: CREATE_TIME={cleaned_data.loc[idx, 'created_at']}, UPDATE_TIME={cleaned_data.loc[idx, 'updated_at']}")
        
        if invalid_records_count > 5:
            context.log.warning(f"  ... è¿˜æœ‰ {invalid_records_count - 5} æ¡ç±»ä¼¼è®°å½•")
        
        # åˆ é™¤ä¸ç¬¦åˆæ—¶é—´é€»è¾‘çš„è®°å½•
        cleaned_data = cleaned_data[~invalid_time_logic]
        context.log.info(f"âœ… å·²åˆ é™¤ {invalid_records_count} æ¡æ—¶é—´é€»è¾‘é”™è¯¯çš„è®°å½•")
    else:
        context.log.info("âœ… æ‰€æœ‰è®°å½•çš„æ—¶é—´é€»è¾‘éƒ½æ­£ç¡®")
    return cleaned_data

def test_null_count(cleaned_data,context):
    for col in cleaned_data.columns:
        null_count = cleaned_data[col].isnull().sum()
        if null_count > 0:
            context.log.warning(f"âš ï¸ åˆ— '{col}' æœ‰ {null_count} ä¸ªç©ºå€¼")
        
        # æ£€æŸ¥å­—ç¬¦ä¸²é•¿åº¦
        if cleaned_data[col].dtype == 'object':
            max_length = cleaned_data[col].astype(str).str.len().max()
            context.log.info(f"ğŸ“ åˆ— '{col}' æœ€å¤§å­—ç¬¦ä¸²é•¿åº¦: {max_length}")

def clean_dataframe_for_excel(clean_data, datetime_fields):
    """
    ä»…åˆ é™¤ä»»æ„åˆ—ä¸­åŒ…å« Excel éæ³•æ§åˆ¶å­—ç¬¦æˆ– '@' çš„æ•´è¡Œï¼›ä¸åšå…¶ä»–ä»»ä½•æ”¶å°¾å¤„ç†ã€‚
    """
    if clean_data is None or len(clean_data) == 0:
        return clean_data

    # Excel ç¦æ­¢çš„æ§åˆ¶å­—ç¬¦ï¼ˆé™¤ \t \n \r å¤–ï¼‰+ '@'
    pattern = r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F-\x9F@]'

    # å…¨è¡¨æ‰«æï¼ˆåŒ…å«æ—¶é—´åˆ—ï¼‰ï¼Œåªåˆ è¡Œä¸æ”¹å€¼
    row_bad = clean_data.astype(str).apply(lambda c: c.str.contains(pattern, regex=True, na=False)).any(axis=1)
    bad_count = int(row_bad.sum())
    if bad_count:
        clean_data = clean_data.loc[~row_bad]
    return clean_data

def clean_invalid_process_field_characters(cleaned_data, process_field, context):
    """
    æ¸…é™¤process_fieldä¸­åŒ…å«éæ³•å­—ç¬¦çš„è®°å½•
    
    Args:
        cleaned_data: æ¸…æ´—åçš„æ•°æ®DataFrame
        process_field: æ¸…æ´—åçš„å­—æ®µå
        context: Dagsteræ‰§è¡Œä¸Šä¸‹æ–‡
    
    Returns:
        cleaned_data: æ¸…é™¤éæ³•è®°å½•åçš„æ•°æ®DataFrame
    """
    context.log.info(f"ğŸ” å¼€å§‹æ£€æŸ¥{process_field}ä¸­çš„éæ³•å­—ç¬¦...")
    
    if process_field not in cleaned_data.columns:
        context.log.warning(f"âš ï¸ {process_field} åˆ—ä¸å­˜åœ¨ï¼Œè·³è¿‡éæ³•å­—ç¬¦æ£€æŸ¥")
        return cleaned_data
    
    original_count = len(cleaned_data)
    
    # ç¡®ä¿process_fieldä¸ºå­—ç¬¦ä¸²ç±»å‹
    cleaned_data[process_field] = cleaned_data[process_field].astype(str)
    
    # å®šä¹‰éæ³•å­—ç¬¦æ¨¡å¼
    illegal_patterns = [
        r'@',                     # @ ç¬¦å·
        r'^\s*[A-Z]\s*$',        # å•ç‹¬çš„å¤§å†™å­—æ¯ï¼ˆå¦‚å•ç‹¬çš„Zï¼‰
        r'[\x00-\x1f\x7f-\x9f]', # æ§åˆ¶å­—ç¬¦
        r'[\t\n\r\f\v]',         # åˆ¶è¡¨ç¬¦ã€æ¢è¡Œç¬¦ç­‰
        r'[#$%^&*()+=\[\]{}|\\:";\'<>?,]', # ç‰¹æ®Šç¬¦å·
        r'^\s*$',                # åªåŒ…å«ç©ºç™½å­—ç¬¦
        r'^NULL$|^null$|^Null$', # NULLå€¼
        r'^\*+$',                # åªåŒ…å«æ˜Ÿå·
        r'^-+$'                  # åªåŒ…å«ç ´æŠ˜å·
    ]
    
    # åˆ›å»ºå¸ƒå°”æ©ç æ ‡è¯†æ— æ•ˆè®°å½•
    invalid_mask = pd.Series([False] * len(cleaned_data), index=cleaned_data.index)
    
    # é€ä¸ªæ£€æŸ¥æ¯ç§éæ³•æ¨¡å¼
    for i, pattern in enumerate(illegal_patterns):
        try:
            pattern_mask = cleaned_data[process_field].str.contains(pattern, regex=True, na=False)
            pattern_count = pattern_mask.sum()
            
            if pattern_count > 0:
                context.log.warning(f"âš ï¸ æ£€æµ‹åˆ° {pattern_count} æ¡åŒ…å«æ¨¡å¼ '{pattern}' çš„è®°å½•")
                
                # æ˜¾ç¤ºåŒ¹é…çš„æ ·æœ¬å€¼
                sample_values = cleaned_data[pattern_mask][process_field].head(3).tolist()
                context.log.warning(f"  æ ·æœ¬å€¼: {sample_values}")
                
                invalid_mask |= pattern_mask
                
        except Exception as e:
            context.log.error(f"âŒ æ£€æŸ¥æ¨¡å¼ '{pattern}' æ—¶å‡ºé”™: {e}")
            continue
    
    # é¢å¤–æ£€æŸ¥ï¼šå¼‚å¸¸é•¿åº¦
    length_mask = (cleaned_data[process_field].str.len() < 2) | (cleaned_data[process_field].str.len() > 50)
    length_count = length_mask.sum()
    
    if length_count > 0:
        context.log.warning(f"âš ï¸ å‘ç° {length_count} æ¡{process_field}é•¿åº¦å¼‚å¸¸çš„è®°å½•ï¼ˆ<2æˆ–>50å­—ç¬¦ï¼‰")
        invalid_mask |= length_mask
    
    # ç»Ÿè®¡å¹¶å¤„ç†æ— æ•ˆè®°å½•
    total_invalid_count = invalid_mask.sum()
    
    if total_invalid_count > 0:
        context.log.warning(f"âš ï¸ æ€»è®¡å‘ç° {total_invalid_count} æ¡åŒ…å«éæ³•{process_field}çš„è®°å½•")
        
        # æ˜¾ç¤ºè¢«åˆ é™¤è®°å½•çš„è¯¦ç»†ä¿¡æ¯
        invalid_records = cleaned_data[invalid_mask]
        context.log.warning("ğŸ“‹ è¢«åˆ é™¤çš„è®°å½•è¯¦æƒ…ï¼ˆå‰5æ¡ï¼‰:")
        
        for i, (idx, row) in enumerate(invalid_records.head(5).iterrows()):
            context.log.warning(f"  è®°å½• {i+1}: ç´¢å¼•={idx}, {process_field}='{row[process_field]}'")
        
        if total_invalid_count > 5:
            context.log.warning(f"  ... è¿˜æœ‰ {total_invalid_count - 5} æ¡ç±»ä¼¼è®°å½•")
        
        # åˆ é™¤æ— æ•ˆè®°å½•
        cleaned_data = cleaned_data[~invalid_mask].copy()
        
        final_count = len(cleaned_data)
        removed_count = original_count - final_count
        cleanup_rate = (removed_count / original_count) * 100 if original_count > 0 else 0
        
        context.log.info(f"âœ… å·²åˆ é™¤ {removed_count} æ¡éæ³•{process_field}è®°å½•")
        context.log.info(f"ğŸ“Š æ¸…ç†ç»Ÿè®¡: åŸå§‹={original_count}æ¡, æ¸…ç†å={final_count}æ¡, æ¸…ç†ç‡={cleanup_rate:.2f}%")
        
        # æ•°æ®è´¨é‡è­¦å‘Š
        if cleanup_rate > 15:
            context.log.warning(f"âš ï¸ æ•°æ®æ¸…ç†ç‡è¾ƒé«˜ ({cleanup_rate:.2f}%)ï¼Œå»ºè®®æ£€æŸ¥ä¸Šæ¸¸æ•°æ®è´¨é‡")
        elif cleanup_rate > 5:
            context.log.info(f"â„¹ï¸ æ•°æ®æ¸…ç†ç‡æ­£å¸¸ ({cleanup_rate:.2f}%)")
            
    else:
        context.log.info(f"âœ… æ‰€æœ‰{process_field}å­—æ®µéƒ½æ˜¯æœ‰æ•ˆçš„ï¼Œæ— éœ€æ¸…ç†")
    
    return cleaned_data

def clean_short_process_field(cleaned_data, process_field, context, min_length=5):
    """
    æ¸…é™¤process_fieldé•¿åº¦ä¸è¶…è¿‡æŒ‡å®šå­—ç¬¦æ•°çš„è®°å½•
    
    Args:
        cleaned_data: æ¸…æ´—åçš„æ•°æ®DataFrame
        process_field: æ¸…æ´—åçš„å­—æ®µå
        context: Dagsteræ‰§è¡Œä¸Šä¸‹æ–‡
        min_length: æœ€å°é•¿åº¦è¦æ±‚ï¼Œé»˜è®¤ä¸º5
    
    Returns:
        cleaned_data: æ¸…é™¤æŒ‡å®šå­—æ®µé•¿åº¦ä¸ç¬¦åˆè¦æ±‚çš„è®°å½•åçš„æ•°æ®DataFrame
    """
    context.log.info(f"ğŸ” å¼€å§‹æ£€æŸ¥{process_field}é•¿åº¦ï¼ˆæœ€å°è¦æ±‚: {min_length}ä¸ªå­—ç¬¦ï¼‰...")
    
    if process_field not in cleaned_data.columns:
        context.log.warning(f"âš ï¸ {process_field} åˆ—ä¸å­˜åœ¨ï¼Œè·³è¿‡é•¿åº¦æ£€æŸ¥")
        return cleaned_data
    
    original_count = len(cleaned_data)
    
    # ç¡®ä¿process_fieldä¸ºå­—ç¬¦ä¸²ç±»å‹
    cleaned_data[process_field] = cleaned_data[process_field].astype(str)
    
    # è®¡ç®—æ¯ä¸ªprocess_fieldçš„é•¿åº¦
    id_lengths = cleaned_data[process_field].str.len()
    
    # æ‰¾å‡ºé•¿åº¦ä¸è¶…è¿‡æŒ‡å®šå­—ç¬¦æ•°çš„è®°å½•
    short_id_mask = id_lengths <= min_length
    short_id_count = short_id_mask.sum()
    
    if short_id_count > 0:
        context.log.warning(f"âš ï¸ å‘ç° {short_id_count} æ¡{process_field}é•¿åº¦ä¸è¶…è¿‡{min_length}ä¸ªå­—ç¬¦çš„è®°å½•")
        
        # æ˜¾ç¤ºè¢«åˆ é™¤è®°å½•çš„è¯¦ç»†ä¿¡æ¯
        short_records = cleaned_data[short_id_mask]
        context.log.warning(f"ğŸ“‹ è¢«åˆ é™¤çš„çŸ­{process_field}è®°å½•è¯¦æƒ…ï¼ˆå‰10æ¡ï¼‰:")
        
        for i, (idx, row) in enumerate(short_records.head(10).iterrows()):
            id_value = row[process_field]
            id_length = len(str(id_value))
            context.log.warning(f"  è®°å½• {i+1}: ç´¢å¼•={idx}, {process_field}='{id_value}', é•¿åº¦={id_length}")
        
        if short_id_count > 10:
            context.log.warning(f"  ... è¿˜æœ‰ {short_id_count - 10} æ¡ç±»ä¼¼è®°å½•")
        
        # ç»Ÿè®¡é•¿åº¦åˆ†å¸ƒï¼ˆç”¨äºåˆ†ææ•°æ®è´¨é‡ï¼‰
        length_distribution = id_lengths[short_id_mask].value_counts().sort_index()
        context.log.info(f"ğŸ“Š çŸ­{process_field}é•¿åº¦åˆ†å¸ƒ:")
        for length, count in length_distribution.items():
            context.log.info(f"  é•¿åº¦ {length}: {count} æ¡è®°å½•")
        
        # åˆ é™¤é•¿åº¦ä¸ç¬¦åˆè¦æ±‚çš„è®°å½•
        cleaned_data = cleaned_data[~short_id_mask].copy()
        
        final_count = len(cleaned_data)
        removed_count = original_count - final_count
        cleanup_rate = (removed_count / original_count) * 100 if original_count > 0 else 0
        
        context.log.info(f"âœ… å·²åˆ é™¤ {removed_count} æ¡é•¿åº¦ä¸è¶…è¿‡{min_length}ä¸ªå­—ç¬¦çš„{process_field}è®°å½•")
        context.log.info(f"ğŸ“Š æ¸…ç†ç»Ÿè®¡: åŸå§‹={original_count}æ¡, æ¸…ç†å={final_count}æ¡, æ¸…ç†ç‡={cleanup_rate:.2f}%")
        
        # æ•°æ®è´¨é‡åˆ†æ
        if cleanup_rate > 20:
            context.log.warning(f"âš ï¸ çŸ­{process_field}æ¸…ç†ç‡å¾ˆé«˜ ({cleanup_rate:.2f}%)ï¼Œè¯·æ£€æŸ¥ä¸Šæ¸¸æ•°æ®è´¨é‡æ ‡å‡†")
        elif cleanup_rate > 10:
            context.log.warning(f"âš ï¸ çŸ­{process_field}æ¸…ç†ç‡è¾ƒé«˜ ({cleanup_rate:.2f}%)ï¼Œå»ºè®®å…³æ³¨æ•°æ®è´¨é‡")
        else:
            context.log.info(f"â„¹ï¸ çŸ­{process_field}æ¸…ç†ç‡æ­£å¸¸ ({cleanup_rate:.2f}%)")
        
        # æ˜¾ç¤ºæ¸…ç†åçš„é•¿åº¦ç»Ÿè®¡
        if final_count > 0:
            remaining_lengths = cleaned_data[process_field].str.len()
            min_remaining = remaining_lengths.min()
            max_remaining = remaining_lengths.max()
            avg_remaining = remaining_lengths.mean()
            context.log.info(f"ğŸ“ æ¸…ç†å{process_field}é•¿åº¦ç»Ÿè®¡: æœ€çŸ­={min_remaining}, æœ€é•¿={max_remaining}, å¹³å‡={avg_remaining:.1f}")
        
    else:
        context.log.info(f"âœ… æ‰€æœ‰{process_field}é•¿åº¦éƒ½ç¬¦åˆè¦æ±‚ï¼ˆ>{min_length}ä¸ªå­—ç¬¦ï¼‰ï¼Œæ— éœ€æ¸…ç†")
        
        # æ˜¾ç¤ºå½“å‰çš„é•¿åº¦ç»Ÿè®¡
        if original_count > 0:
            min_length_current = id_lengths.min()
            max_length_current = id_lengths.max()
            avg_length_current = id_lengths.mean()
            context.log.info(f"ğŸ“ å½“å‰{process_field}é•¿åº¦ç»Ÿè®¡: æœ€çŸ­={min_length_current}, æœ€é•¿={max_length_current}, å¹³å‡={avg_length_current:.1f}")
    
    return cleaned_data